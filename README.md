# Transformations in Regression: Repository Overview

This repository showcases several projects that explore how transformations can improve regression models in different contexts. Each analysis demonstrates why a simple linear model might fall short, how diagnostic checks guide the choice of transformation, and how techniques like polynomial terms, log-transforms, and generalized least squares can address issues of non-linearity or non-constant variance. The data used here range from agricultural field experiments to vapor pressure measurements, tree volume records, and custom rubber band trials, giving a broad view of when and why transformations can help.

In the Corn Yield Analysis, we look at nitrogen fertilizer application in Wisconsin and how corn yield responds. After noticing heteroscedastic and autocorrelated residuals, various transformations (including Box-Cox) and correlation structures (via GLS) are introduced to stabilize variance and improve model reliability. In the Temperature–Vapor Pressure project, we explore R’s built-in `pressure` dataset and show how an exponential relationship is linearized through a log-transform, then refine it further by addressing autocorrelated errors with generalized least squares. The Polynomial and Transformation-Based Regression on Tree Volume project uses the `trees` dataset to compare ordinary linear models, log- and sqrt-transformations, and polynomial expansions. It highlights how transformations or polynomial terms can better capture curvature and produce more accurate predictions. Finally, the Rubber Band Regression Analysis walks through a practical experiment where rubber bands are incrementally loaded with weights, revealing curved relationships that benefit from a quadratic transformation and carefully interpreted confidence and prediction intervals.

Most of these projects rely on common R packages such as **dplyr**, **MASS**, **nlme**, **performance**, and **boot**, alongside base plotting and diagnostic functions. In a few cases, code examples also appear in a Jupyter notebook format, though the core statistical routines remain the same. These libraries facilitate data wrangling, model fitting, residual diagnostics, and advanced regression methods like Box-Cox transformations, generalized least squares, and polynomial expansions. By comparing different approaches and visually inspecting model diagnostics, each project demonstrates how transformations can resolve issues like non-linearity, heteroskedasticity, and serial correlation while guiding best practices in data analysis.

Please feel free to browse each project folder to see the accompanying code, extended write-ups, and figures. All files are self-contained and can be run in R (or a Jupyter environment with an R kernel) to reproduce the results. This repository aims to serve as a reference point for applying transformations in regression and a collection of examples that illustrate how even simple adjustments can yield substantial improvements in model fit and interpretability.
